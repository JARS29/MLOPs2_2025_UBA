{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on - Procesamiento de Streaming para ML con Apache Kafka y Python\n",
    "\n",
    "En este hands-on, exploraremos cómo trabajar con flujos de datos en tiempo real utilizando Apache Kafka como broker de mensajes y Python para el procesamiento. Simularemos un escenario donde se generan eventos de datos, se ingieren a través de Kafka, y luego se procesan en tiempo real para realizar inferencias con un modelo de Machine Learning pre-entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requisitos Previos\n",
    "\n",
    "Asegúrate de tener instalados los siguientes componentes en tu sistema:\n",
    "\n",
    "* **Docker y Docker Compose:** Para levantar el servicio de Kafka de manera sencilla.\n",
    "    * [Instalar Docker](https://docs.docker.com/get-docker/)\n",
    "    * [Instalar Docker Compose](https://docs.docker.com/compose/install/)\n",
    "* **Python 3.8+:** Para ejecutar los scripts de productor y consumidor.\n",
    "    * [Descargar Python](https://www.python.org/downloads/)\n",
    "* **Poetry:** Para gestionar las dependencias del proyecto Python.\n",
    "    * [Instalar Poetry](https://python-poetry.org/docs/#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructura del Proyecto\n",
    "\n",
    "Vamos a organizar los archivos de la siguiente manera. Crea una carpeta principal para el hands-on, por ejemplo, `hands_on_streaming`. Dentro de ella, crearemos los archivos.\n",
    "\n",
    "```\n",
    "streaming1/\n",
    "├── docker-compose.yml\n",
    "├── producer.py\n",
    "├── consumer.py\n",
    "├── ml_model/\n",
    "│   └── simple_model.pkl\n",
    "├── pyproject.toml\n",
    "└── poetry.lock\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración del Entorno (Kafka con Docker Compose)\n",
    "\n",
    "Primero, crearemos el archivo `docker-compose.yml` para levantar un servidor Kafka y Zookeeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3.1: Crear el archivo `docker-compose.yml`**\n",
    "\n",
    "Crea un archivo llamado `docker-compose.yml` en la raíz de tu carpeta `streaming1/` y pega el siguiente contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.0.1\n",
    "    hostname: zookeeper\n",
    "    container_name: zookeeper\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.0.1\n",
    "    hostname: kafka\n",
    "    container_name: kafka\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "      - \"9094:9094\" # Puerto para conexión externa\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    depends_on:\n",
    "      - zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3.2: Levantar Kafka**\n",
    "\n",
    "Abre una terminal (no desde Jupyter, ya que necesitamos un proceso en segundo plano), navega hasta la carpeta `streaming1/` y ejecuta el siguiente comando para levantar los servicios de Kafka y Zookeeper en segundo plano:\n",
    "\n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "Puedes verificar que los contenedores estén corriendo con `docker compose ps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparación del Modelo de Machine Learning\n",
    "\n",
    "Para este hands-on, usaremos un modelo muy simple pre-entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4.1: Crear la carpeta `ml_model`**\n",
    "\n",
    "Puedes crearla directamente con el siguiente comando en una celda de código (asegúrate de que el directorio `streaming1/ml_model` exista):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('streaming1/ml_model', exist_ok=True)\n",
    "print(\"Carpeta 'streaming1/ml_model' creada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4.2: Generar y guardar un modelo simple**\n",
    "\n",
    "Ejecuta la siguiente celda de código para generar el modelo y guardarlo. Asegúrate de tener `scikit-learn` y `joblib` instalados en tu entorno de Jupyter. Si no, puedes instalarlos con `!pip install scikit-learn joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Datos dummy para entrenar un modelo simple\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Guardar el modelo en la ruta esperada por el consumidor\n",
    "joblib.dump(model, 'simple_model.pkl')\n",
    "print(\"Modelo simple guardado como streaming1/ml_model/simple_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuración de Dependencias con Poetry\n",
    "\n",
    "Usaremos Poetry para gestionar las dependencias de Python.\n",
    "\n",
    "**Paso 5.1: Inicializar Poetry y añadir dependencias**\n",
    "\n",
    "Abre una terminal (no desde Jupyter) en la carpeta `streaming1/` y ejecuta los siguientes comandos:\n",
    "\n",
    "```bash\n",
    "cd streaming1/\n",
    "poetry init --no-interaction\n",
    "poetry add kafka-python pandas scikit-learn\n",
    "```\n",
    "\n",
    "Esto creará los archivos `pyproject.toml` y `poetry.lock` en tu directorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementación del Productor de Datos\n",
    "\n",
    "El productor generará eventos (simulando datos de sensores o métricas) y los enviará a un tópico de Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 6.1: Crear el archivo `producer.py`**\n",
    "\n",
    "Crea un archivo llamado `producer.py` en la raíz de tu carpeta `streaming1/` y pega el siguiente contenido. Luego, no olvides guardar el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hands_on_streaming/producer.py\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def serialize_json(obj):\n",
    "    return json.dumps(obj).encode('utf-8')\n",
    "\n",
    "# Configuración del productor Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9094'], # Usar el puerto expuesto por Docker Compose\n",
    "    value_serializer=serialize_json\n",
    ")\n",
    "\n",
    "topic_name = 'sensor_data'\n",
    "\n",
    "print(f\"Enviando datos al tópico: {topic_name}\")\n",
    "\n",
    "try:\n",
    "    for i in range(100):\n",
    "        sensor_id = random.randint(1, 5)\n",
    "        temperature = round(random.uniform(20.0, 30.0), 2)\n",
    "        humidity = round(random.uniform(40.0, 60.0), 2)\n",
    "        pressure = round(random.uniform(900.0, 1100.0), 2)\n",
    "\n",
    "        data = {\n",
    "            'sensor_id': sensor_id,\n",
    "            'timestamp': time.time(),\n",
    "            'temperature': temperature,\n",
    "            'humidity': humidity,\n",
    "            'pressure': pressure\n",
    "        }\n",
    "\n",
    "        producer.send(topic_name, value=data)\n",
    "        print(f\"Sent: {data}\")\n",
    "        time.sleep(1) # Enviar un evento cada segundo\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deteniendo productor...\")\n",
    "finally:\n",
    "    producer.close()\n",
    "    print(\"Productor cerrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementación del Consumidor y Procesamiento ML en Tiempo Real\n",
    "\n",
    "El consumidor leerá los eventos del tópico de Kafka, realizará un preprocesamiento básico y usará el modelo de ML para hacer inferencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 7.1: Crear el archivo `consumer.py`**\n",
    "\n",
    "Crea un archivo llamado `consumer.py` en la raíz de tu carpeta `streaming1/` y pega el siguiente contenido. Luego, no olvides guardar el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hands_on_streaming/consumer.py\n",
    "import json\n",
    "import joblib\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo pre-entrenado\n",
    "model = joblib.load('ml_model/simple_model.pkl')\n",
    "print(\"Modelo de ML cargado exitosamente.\")\n",
    "\n",
    "# Configuración del consumidor Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    'sensor_data',\n",
    "    bootstrap_servers=['localhost:9094'], # Usar el puerto expuesto por Docker Compose\n",
    "    auto_offset_reset='earliest', # Empieza a leer desde el principio si no hay offset guardado\n",
    "    enable_auto_commit=True,\n",
    "    group_id='ml-processing-group',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(\"Escuchando mensajes en el tópico 'sensor_data'...\")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        data = message.value\n",
    "        print(f\"Received: {data}\")\n",
    "\n",
    "        # Simular preprocesamiento de features\n",
    "        # Para nuestro modelo simple, usaremos temperatura y humedad como features\n",
    "        features = np.array([[data['temperature'], data['humidity']]])\n",
    "        \n",
    "        # Realizar inferencia\n",
    "        prediction = model.predict(features)[0]\n",
    "        prediction_proba = model.predict_proba(features)[0].tolist()\n",
    "\n",
    "        print(f\"  Processed features: {features.tolist()}\")\n",
    "        print(f\"  Prediction: {'Anomalía' if prediction == 1 else 'Normal'}\")\n",
    "        print(f\"  Prediction probabilities: {prediction_proba}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deteniendo consumidor...\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "    print(\"Consumidor cerrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejecución del Hands-on\n",
    "\n",
    "Para ver el flujo de datos en acción, necesitarás abrir dos terminales separadas, ambas navegando a la carpeta `streaming1/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 8.1: Iniciar el Consumidor (Terminal 1)**\n",
    "\n",
    "En tu primera terminal, navega a la carpeta `streaming1/` y ejecuta el consumidor. Este estará a la espera de mensajes.\n",
    "\n",
    "```bash\n",
    "cd streaming1/\n",
    "poetry run python consumer.py\n",
    "```\n",
    "\n",
    "Deberías ver el mensaje: \"Escuchando mensajes en el tópico 'sensor_data'...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 8.2: Iniciar el Productor (Terminal 2)**\n",
    "\n",
    "En una **segunda terminal**, también navega a la carpeta `streaming1/` y ejecuta el productor. Este comenzará a enviar datos a Kafka.\n",
    "\n",
    "```bash\n",
    "cd streaming1/\n",
    "poetry run python producer.py\n",
    "```\n",
    "\n",
    "Verás cómo el productor envía mensajes en la Terminal 2. Simultáneamente, en la Terminal 1 (donde corre el consumidor), deberías ver cómo cada mensaje es recibido, preprocesado y se realiza una predicción.\n",
    "\n",
    "¡Felicidades! Has implementado un sistema simple de procesamiento de streaming con Kafka y Python para realizar inferencia de Machine Learning en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Limpieza\n",
    "\n",
    "Una vez que hayas terminado con el hands-on, es importante detener los servicios de Docker para liberar recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 9.1: Detener los contenedores de Docker**\n",
    "\n",
    "En la carpeta `streaming1/`, ejecuta:\n",
    "\n",
    "```bash\n",
    "docker compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 9.2: Limpieza completa (opcional)**\n",
    "\n",
    "Si deseas eliminar también las imágenes de Docker y los volúmenes para una limpieza más profunda, puedes usar:\n",
    "\n",
    "```bash\n",
    "docker compose down --rmi all --volumes\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
