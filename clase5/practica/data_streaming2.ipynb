{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on - Procesamiento de Streaming para ML con Apache Spark y Kafka\n",
    "\n",
    "En este hands-on, exploraremos cómo trabajar con flujos de datos en tiempo real utilizando Apache Kafka como broker de mensajes y Apache Spark Structured Streaming para el procesamiento y la inferencia de Machine Learning. Simularemos un escenario donde se generan eventos de datos, se ingieren a través de Kafka, y luego Spark procesa estos datos en tiempo real para realizar inferencias con un modelo de ML pre-entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requisitos Previos\n",
    "\n",
    "Asegúrate de tener instalados los siguientes componentes en tu sistema:\n",
    "\n",
    "* **Docker y Docker Compose:** Para levantar el servicio de Kafka de manera sencilla.\n",
    "    * [Instalar Docker](https://docs.docker.com/get-docker/)\n",
    "    * [Instalar Docker Compose](https://docs.docker.com/compose/install/)\n",
    "* **Java Development Kit (JDK) 8 o superior:** Requerido por Apache Spark. **Importante:** Descarga e instala el JDK antes de instalar Spark.\n",
    "    * [Descargar JDK](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html) (o versión más reciente)\n",
    "* **Python 3.8+:** Para ejecutar los scripts de productor y el entorno PySpark.\n",
    "    * [Descargar Python](https://www.python.org/downloads/)\n",
    "* **Poetry:** Para gestionar las dependencias del proyecto Python.\n",
    "    * [Instalar Poetry](https://python-poetry.org/docs/#installation)\n",
    "* **Apache Spark (Opcional, pero recomendado para el entorno):** Aunque `pyspark` se instalará con Poetry, tener una instalación local de Spark puede ayudar con la configuración de `SPARK_HOME`.\n",
    "    * [Descargar Spark](https://spark.apache.org/downloads.html) (versión compatible con tu Python y Scala/Java)\n",
    "    * **Para Windows:** Descarga la versión pre-compilada de Spark y descomprímela en una ruta simple (ej. `C:\\spark`). También necesitarás descargar `winutils.exe` (puedes buscar \"winutils.exe hadoop version\" para encontrar una compatible con tu Spark y Hadoop).\n",
    "        Coloca `winutils.exe` en `SPARK_HOME/bin` o `HADOOP_HOME/bin` si configuras `HADOOP_HOME`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de Variables de Entorno para Spark\n",
    "\n",
    "Es crucial que las siguientes variables de entorno estén configuradas correctamente para que PySpark funcione. Si has instalado Spark localmente, asegúrate de que `SPARK_HOME` apunte a tu directorio de instalación de Spark.\n",
    "\n",
    "#### Para Linux/macOS (añadir a `~/.bashrc` o `~/.zshrc`):\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=\"/path/to/your/jdk/installation\"\n",
    "export SPARK_HOME=\"/path/to/your/spark/installation\"\n",
    "export PATH=\"$PATH:$JAVA_HOME/bin:$SPARK_HOME/bin\"\n",
    "export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH\"\n",
    "```\n",
    "\n",
    "#### Para Windows:\n",
    "\n",
    "1.  **JAVA_HOME:**\n",
    "    * Ve a `Panel de Control` -> `Sistema y Seguridad` -> `Sistema` -> `Configuración avanzada del sistema` -> `Variables de entorno`.\n",
    "    * En `Variables del sistema`, haz clic en `Nueva...`.\n",
    "    * **Nombre de la variable:** `JAVA_HOME`\n",
    "    * **Valor de la variable:** `C:\\Program Files\\Java\\jdk-XX.X.X` (reemplaza con tu ruta de instalación de JDK).\n",
    "2.  **SPARK_HOME:**\n",
    "    * En `Variables del sistema`, haz clic en `Nueva...`.\n",
    "    * **Nombre de la variable:** `SPARK_HOME`\n",
    "    * **Valor de la variable:** `C:\\spark` (reemplaza con tu ruta de instalación de Spark).\n",
    "3.  **Path:**\n",
    "    * En `Variables del sistema`, busca la variable `Path` y haz clic en `Editar...`.\n",
    "    * Haz clic en `Nueva` y añade `%JAVA_HOME%\\bin`.\n",
    "    * Haz clic en `Nueva` y añade `%SPARK_HOME%\\bin`.\n",
    "    * **Opcional (para PySpark):** Si usas un entorno Python específico, puedes configurar `PYSPARK_PYTHON`. Por ejemplo, `C:\\Users\\YourUser\\AppData\\Local\\Programs\\Python\\Python39\\python.exe` (ajusta tu ruta de Python).\n",
    "        * **Nombre de la variable:** `PYSPARK_PYTHON`\n",
    "        * **Valor de la variable:** `C:\\path\\to\\your\\python.exe`\n",
    "4.  Haz clic en `Aceptar` en todas las ventanas para guardar los cambios.\n",
    "\n",
    "Reinicia tu terminal (o Visual Studio Code, etc.) después de configurar estas variables para que los cambios surtan efecto. Si estás usando un entorno virtual de Poetry, PySpark intentará encontrar Spark por sí mismo si `SPARK_HOME` está configurado globalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructura del Proyecto\n",
    "\n",
    "Vamos a organizar los archivos de la siguiente manera. Crea una carpeta principal para el hands-on, llamada `streaming2`. Dentro de ella, crearemos los archivos.\n",
    "\n",
    "```\n",
    "streaming2/\n",
    "├── docker-compose.yml\n",
    "├── producer.py\n",
    "├── spark_consumer.py\n",
    "├── ml_model/\n",
    "│   └── simple_model.pkl\n",
    "├── pyproject.toml\n",
    "└── poetry.lock\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración del Entorno (Kafka con Docker Compose)\n",
    "\n",
    "Primero, crearemos el archivo `docker-compose.yml` para levantar un servidor Kafka y Zookeeper. Spark Structured Streaming se conectará a este Kafka para consumir los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3.1: Crear el archivo `docker-compose.yml`**\n",
    "\n",
    "Crea un archivo llamado `docker-compose.yml` en la raíz de tu carpeta `streaming2/` y pega el siguiente contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streaming2/docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.0.1\n",
    "    hostname: zookeeper\n",
    "    container_name: zookeeper\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.0.1\n",
    "    hostname: kafka\n",
    "    container_name: kafka\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "      - \"9094:9094\" # Puerto para conexión externa\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    depends_on:\n",
    "      - zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3.2: Levantar Kafka**\n",
    "\n",
    "Abre una terminal (no desde Jupyter, ya que necesitamos un proceso en segundo plano), navega hasta la carpeta `streaming2/` y ejecuta el siguiente comando para levantar los servicios de Kafka y Zookeeper en segundo plano:\n",
    "\n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "Puedes verificar que los contenedores estén corriendo con `docker compose ps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparación del Modelo de Machine Learning\n",
    "\n",
    "Para este hands-on, usaremos un modelo muy simple pre-entrenado. Spark puede cargar modelos de diferentes formatos, pero `joblib` es común para modelos de `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4.1: Crear la carpeta `ml_model`**\n",
    "\n",
    "Puedes crearla directamente con el siguiente comando en una celda de código (asegúrate de que el directorio `streaming2` exista):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('streaming2/ml_model', exist_ok=True)\n",
    "print(\"Carpeta 'streaming2/ml_model' creada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4.2: Generar y guardar un modelo simple**\n",
    "\n",
    "Ejecuta la siguiente celda de código para generar el modelo y guardarlo. Asegúrate de tener `scikit-learn` y `joblib` instalados en tu entorno de Jupyter. Si no, puedes instalarlos con `!pip install scikit-learn joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Datos dummy para entrenar un modelo simple\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Guardar el modelo en la ruta esperada por el consumidor de Spark\n",
    "joblib.dump(model, 'streaming2/ml_model/simple_model.pkl')\n",
    "print(\"Modelo simple guardado como streaming2/ml_model/simple_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuración de Dependencias con Poetry\n",
    "\n",
    "Usaremos Poetry para gestionar las dependencias de Python, incluyendo `pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 5.1: Inicializar Poetry y añadir dependencias**\n",
    "\n",
    "Desde la terminal, asegúrate de estar en la carpeta `streaming2/` y ejecuta los siguientes comandos:\n",
    "\n",
    "```bash\n",
    "cd streaming2/\n",
    "poetry init --no-interaction\n",
    "poetry add kafka-python pandas scikit-learn pyspark\n",
    "```\n",
    "\n",
    "Esto creará los archivos `pyproject.toml` y `poetry.lock` en tu directorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementación del Productor de Datos\n",
    "\n",
    "El productor generará eventos (simulando datos de sensores o métricas) y los enviará a un tópico de Kafka. Este es el mismo productor que en el ejemplo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 6.1: Crear el archivo `producer.py`**\n",
    "\n",
    "Crea un archivo llamado `producer.py` en la raíz de tu carpeta `streaming2/` y pega el siguiente contenido. Luego, no olvides guardar el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streaming2/producer.py\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def serialize_json(obj):\n",
    "    return json.dumps(obj).encode('utf-8')\n",
    "\n",
    "# Configuración del productor Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9094'], # Usar el puerto expuesto por Docker Compose\n",
    "    value_serializer=serialize_json\n",
    ")\n",
    "\n",
    "topic_name = 'sensor_data'\n",
    "\n",
    "print(f\"Enviando datos al tópico: {topic_name}\")\n",
    "\n",
    "try:\n",
    "    for i in range(100):\n",
    "        sensor_id = random.randint(1, 5)\n",
    "        temperature = round(random.uniform(20.0, 30.0), 2)\n",
    "        humidity = round(random.uniform(40.0, 60.0), 2)\n",
    "        pressure = round(random.uniform(900.0, 1100.0), 2)\n",
    "\n",
    "        data = {\n",
    "            'sensor_id': sensor_id,\n",
    "            'timestamp': time.time(),\n",
    "            'temperature': temperature,\n",
    "            'humidity': humidity,\n",
    "            'pressure': pressure\n",
    "        }\n",
    "\n",
    "        producer.send(topic_name, value=data)\n",
    "        print(f\"Sent: {data}\")\n",
    "        time.sleep(1) # Enviar un evento cada segundo\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deteniendo productor...\")\n",
    "finally:\n",
    "    producer.close()\n",
    "    print(\"Productor cerrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementación del Consumidor y Procesamiento ML en Tiempo Real con Apache Spark\n",
    "\n",
    "Este script de PySpark leerá los datos del tópico Kafka, los parseará, aplicará un modelo de ML para la inferencia y mostrará los resultados. Utilizaremos una UDF (User-Defined Function) para cargar el modelo y hacer predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 7.1: Crear el archivo `spark_consumer.py`**\n",
    "\n",
    "Crea un archivo llamado `spark_consumer.py` en la raíz de tu carpeta `streaming2/` y pega el siguiente contenido. No olvides guardar el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streaming2/spark_consumer.py\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, ArrayType\n",
    "\n",
    "# Inicializar SparkSession\n",
    "# Es importante incluir los paquetes de Kafka para Spark Structured Streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkKafkaMLStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\") \\ # Ajusta la versión de Spark si es necesario\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"SparkSession creada.\")\n",
    "\n",
    "# Definir el esquema de los datos que esperamos de Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", LongType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Función para cargar el modelo (se ejecuta en cada worker/executor)\n",
    "model = None # Global para que se cargue una vez por proceso\n",
    "def load_model():\n",
    "    global model\n",
    "    if model is None:\n",
    "        # La ruta del modelo debe ser accesible para los workers de Spark\n",
    "        # Si se ejecuta en un cluster, este archivo debe ser distribuido a los workers\n",
    "        model = joblib.load('streaming2/ml_model/simple_model.pkl')\n",
    "        print(\"Modelo de ML cargado en el worker.\")\n",
    "    return model\n",
    "\n",
    "# UDF para realizar la inferencia\n",
    "def predict_udf(temperature, humidity):\n",
    "    local_model = load_model()\n",
    "    features = np.array([[temperature, humidity]])\n",
    "    prediction = local_model.predict(features)[0]\n",
    "    prediction_proba = local_model.predict_proba(features)[0].tolist()\n",
    "    return int(prediction), prediction_proba\n",
    "\n",
    "# Registrar la UDF. El tipo de retorno es un StructType con la predicción y las probabilidades.\n",
    "prediction_schema = StructType([\n",
    "    StructField(\"prediction\", LongType(), True),\n",
    "    StructField(\"prediction_proba\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "predict_spark_udf = udf(predict_udf, prediction_schema)\n",
    "\n",
    "# Leer datos de Kafka con Structured Streaming\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9094\") \\\n",
    "    .option(\"subscribe\", \"sensor_data\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Conectado a Kafka para streaming.\")\n",
    "\n",
    "# Parsear el valor JSON y aplicar la UDF\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "    .select(from_json(col(\"json_data\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Aplicar la UDF para obtener las predicciones\n",
    "prediction_df = parsed_df.withColumn(\n",
    "    \"ml_result\", \n",
    "    predict_spark_udf(col(\"temperature\"), col(\"humidity\"))\n",
    ").select(\n",
    "    col(\"sensor_id\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"temperature\"),\n",
    "    col(\"humidity\"),\n",
    "    col(\"ml_result.prediction\").alias(\"prediction\"),\n",
    "    col(\"ml_result.prediction_proba\").alias(\"prediction_proba\")\n",
    ")\n",
    "\n",
    "# Iniciar el query de streaming y mostrar los resultados en la consola\n",
    "query = prediction_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming iniciado. Esperando datos...\")\n",
    "\n",
    "# Esperar hasta que el query termine (por ejemplo, por KeyboardInterrupt)\n",
    "query.awaitTermination()\n",
    "print(\"Streaming detenido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejecución del Hands-on\n",
    "\n",
    "Para ver el flujo de datos en acción, necesitarás abrir dos terminales separadas, ambas navegando a la carpeta `streaming2/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 8.1: Iniciar el Consumidor de Spark (Terminal 1)**\n",
    "\n",
    "En tu primera terminal, navega a la carpeta `streaming2/` y ejecuta el consumidor de Spark. Asegúrate de que tu entorno Poetry esté activado o usa `poetry run`.\n",
    "\n",
    "```bash\n",
    "cd streaming2/\n",
    "poetry run python spark_consumer.py\n",
    "```\n",
    "\n",
    "Spark tardará un momento en inicializarse. Deberías ver los mensajes de Spark y finalmente \"Streaming iniciado. Esperando datos...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 8.2: Iniciar el Productor (Terminal 2)**\n",
    "\n",
    "En una **segunda terminal**, también navega a la carpeta `streaming2/` y ejecuta el productor. Este comenzará a enviar datos a Kafka.\n",
    "\n",
    "```bash\n",
    "cd streaming2/\n",
    "poetry run python producer.py\n",
    "```\n",
    "\n",
    "Verás cómo el productor envía mensajes en la Terminal 2. Simultáneamente, en la Terminal 1 (donde corre el consumidor de Spark), deberías ver los micro-batches de datos siendo procesados por Spark y las predicciones de ML impresas en la consola.\n",
    "\n",
    "¡Felicidades! Has implementado un sistema de procesamiento de streaming robusto con Kafka y Apache Spark para realizar inferencia de Machine Learning en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Limpieza\n",
    "\n",
    "Una vez que hayas terminado con el hands-on, es importante detener los servicios de Docker para liberar recursos y detener el proceso de Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 9.1: Detener los procesos de Python (Productor y Consumidor)**\n",
    "\n",
    "En ambas terminales donde ejecutaste el `producer.py` y `spark_consumer.py`, presiona `Ctrl+C` para detener los scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 9.2: Detener los contenedores de Docker**\n",
    "\n",
    "En la carpeta `streaming2/`, ejecuta:\n",
    "\n",
    "```bash\n",
    "docker compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 9.3: Limpieza completa (opcional)**\n",
    "\n",
    "Si deseas eliminar también las imágenes de Docker y los volúmenes para una limpieza más profunda, puedes usar:\n",
    "\n",
    "```bash\n",
    "docker compose down --rmi all --volumes\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python-script",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}