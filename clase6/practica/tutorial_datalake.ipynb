{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Práctico: MLOps Básico en la Nube\n",
    "\n",
    "¡Hola! Bienvenido a esta guía paso a paso para construir tu primer flujo de trabajo de Machine Learning en la nube. No necesitas tener experiencia previa con la nube o GCP. Te guiaremos desde cero. Al final de este tutorial, habrás aprendido a:\n",
    "\n",
    "1.  **Configurar tu entorno de trabajo** en la nube.\n",
    "2.  **Crear un Data Lake** para almacenar tus datos.\n",
    "3.  **Entrenar y guardar un modelo** de Machine Learning.\n",
    "4.  **Desplegar tu modelo** como un servicio web accesible a través de internet.\n",
    "\n",
    "Todo esto se realizará usando los servicios de Google Cloud Platform (GCP), una de las plataformas de computación en la nube más populares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuración Inicial del Entorno (Paso a Paso)\n",
    "\n",
    "Antes de empezar con el Machine Learning, necesitamos preparar nuestro entorno de trabajo. Si ya tienes el SDK de Google Cloud instalado, puedes saltar esta sección.\n",
    "\n",
    "### 0.1. Instalar el SDK de Google Cloud\n",
    "\n",
    "El SDK (Software Development Kit) de Google Cloud es un conjunto de herramientas que te permiten interactuar con los servicios de GCP desde tu terminal. Lo usaremos para crear recursos y desplegar nuestro modelo. \n",
    "\n",
    "* **Descarga:** Ve a la [página de descarga del SDK](https://cloud.google.com/sdk/docs/install) y sigue las instrucciones para tu sistema operativo (Windows, macOS, Linux).\n",
    "* **Configuración:** Una vez instalado, abre una terminal o línea de comandos y ejecuta el siguiente comando para inicializar el SDK y autenticarte con tu cuenta de Google.\n",
    "\n",
    "```bash\n",
    "gcloud init\n",
    "```\n",
    "Este comando te guiará para elegir una cuenta de Google y un proyecto de GCP. Si no tienes un proyecto, te preguntará si quieres crear uno. Elige la opción de crear un proyecto nuevo y dale un nombre único (por ejemplo, `mlops-curso-posgrado-tu-nombre`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Creación y Configuración del Proyecto\n",
    "\n",
    "Un error común es no tener un proyecto creado o no configurar `gcloud` para que apunte al proyecto correcto. Vamos a asegurarnos de que esto esté bien configurado.\n",
    "\n",
    "#### **1. Crea un nuevo proyecto (si no lo hiciste con `gcloud init`):**\n",
    "\n",
    "Elige un `PROJECT_ID` que sea único a nivel global y solo contenga letras, números y guiones. Es una buena práctica usar un prefijo para tus proyectos. Si el comando falla, asegúrate de que el ID sea único y de que la cuenta activa tenga los permisos necesarios.\n",
    "\n",
    "* **Para Terminales Bash (Linux/macOS/Git Bash):**\n",
    "```bash\n",
    "PROJECT_ID=\"mlops-curso-test-tu-nombre\"\n",
    "gcloud projects create $PROJECT_ID\n",
    "```\n",
    "\n",
    "* **Para Terminal de Windows (CMD):**\n",
    "```bat\n",
    "set PROJECT_ID=mlops-curso-test-tu-nombre\n",
    "gcloud projects create %PROJECT_ID%\n",
    "```\n",
    "\n",
    "**Nota:** `gcloud` confirmará que el proyecto se ha creado. Puede tardar unos minutos en aparecer en tu lista de proyectos. Puedes verificarlo con `gcloud projects list`.\n",
    "\n",
    "#### **2. Configura `gcloud` para usar tu proyecto:**\n",
    "\n",
    "Una vez que el proyecto esté creado, ejecuta el siguiente comando para que todas las operaciones de `gcloud` apunten a tu nuevo proyecto.\n",
    "\n",
    "* **Para Terminales Bash (Linux/macOS/Git Bash):**\n",
    "```bash\n",
    "gcloud config set project $PROJECT_ID\n",
    "echo \"El proyecto $PROJECT_ID ha sido configurado como el proyecto por defecto.\"\n",
    "```\n",
    "\n",
    "* **Para Terminal de Windows (CMD):**\n",
    "```bat\n",
    "gcloud config set project %PROJECT_ID%\n",
    "echo El proyecto %PROJECT_ID% ha sido configurado como el proyecto por defecto.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Habilitar Facturación y APIs de GCP\n",
    "\n",
    "**Nota Importante:** Para poder usar la mayoría de los servicios de GCP, incluso en el nivel gratuito, necesitas tener una cuenta de facturación vinculada a tu proyecto. Si ves un error de \"Billing not found\", ve a la [página de facturación de GCP](https://console.cloud.google.com/billing) y vincula una cuenta.\n",
    "\n",
    "Una vez que la facturación esté configurada, puedes habilitar los APIs necesarios para este tutorial:\n",
    "\n",
    "```bash\n",
    "gcloud services enable storage.googleapis.com\n",
    "gcloud services enable run.googleapis.com\n",
    "gcloud services enable cloudbuild.googleapis.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Instalación de las Librerías de Python\n",
    "\n",
    "Los scripts de Python en este tutorial requieren que varias librerías estén instaladas. El error `ModuleNotFoundError` significa que Python no puede encontrar una de estas librerías. Para evitarlo, ejecuta el siguiente comando en tu terminal para instalar todas las dependencias necesarias:\n",
    "\n",
    "```bash\n",
    "pip install google-cloud-storage pandas scikit-learn fastapi uvicorn gunicorn pydantic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5. Autenticación de las Librerías de Python (ADC)\n",
    "\n",
    "Aunque ya te has autenticado con `gcloud auth login`, las librerías de Python necesitan una autenticación separada, llamada **Application Default Credentials (ADC)**. Si no haces este paso, tus scripts de Python no podrán conectarse a los servicios de GCP y verás un error de \"DefaultCredentialsError\".\n",
    "\n",
    "Ejecuta el siguiente comando en tu terminal para configurar las credenciales de la aplicación:\n",
    "\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "Este comando abrirá una ventana en tu navegador para que inicies sesión. Usa la misma cuenta de Google que estás usando para tu proyecto de GCP. Una vez que inicies sesión, las credenciales se guardarán automáticamente para que los scripts de Python puedan usarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación del Data Lake en Google Cloud Storage\n",
    "\n",
    "Un **Data Lake** es un lugar para almacenar grandes cantidades de datos sin procesar. Piensa en él como un disco duro muy grande en la nube. En GCP, usamos **Google Cloud Storage (GCS)** para esto.\n",
    "\n",
    "### 1.1. Crear un 'Bucket' de Almacenamiento\n",
    "\n",
    "En GCS, el contenedor principal para archivos se llama 'bucket'. El nombre del bucket debe ser único en todo el mundo.\n",
    "\n",
    "* **Para Terminales Bash (Linux/macOS/Git Bash):**\n",
    "```bash\n",
    "BUCKET_NAME=\"mlops-data-lake-tutorial-$(date +%s)\"\n",
    "REGION=\"us-central1\" # Puedes elegir la región más cercana a ti\n",
    "gsutil mb -p $PROJECT_ID -l $REGION gs://$BUCKET_NAME\n",
    "echo \"Bucket creado: gs://$BUCKET_NAME\"\n",
    "```\n",
    "\n",
    "* **Para Terminal de Windows (CMD):**\n",
    "```bat\n",
    "rem Para generar un nombre de bucket único, debes hacerlo manualmente. \n",
    "rem Por ejemplo, \"mlops-data-lake-tutorial-2023-10-27\"\n",
    "set BUCKET_NAME=mlops-data-lake-tutorial-tu-nombre-unico\n",
    "set REGION=us-central1\n",
    "gsutil mb -p %PROJECT_ID% -l %REGION% gs://%BUCKET_NAME%\n",
    "echo Bucket creado: gs://%BUCKET_NAME%\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "- `BUCKET_NAME`: Creamos una variable con un nombre único para nuestro bucket.\n",
    "- `gsutil mb`: Este comando crea (`mb` de *make bucket*) el bucket en GCS. \n",
    "- `-p $PROJECT_ID`: Le pasamos explícitamente el ID del proyecto que configuramos antes para que sepa dónde crearlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Simular Carga de Datos\n",
    "\n",
    "Vamos a simular que recibimos un dataset. Usaremos el famoso dataset de Iris de scikit-learn y lo guardaremos en nuestro Data Lake.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# 1. Cargar el dataset de Iris en memoria\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "# 2. Guardar el dataset en un archivo local\n",
    "file_path = \"iris_raw.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Dataset guardado localmente en {file_path}\")\n",
    "print(\"Primeras 5 filas del dataset:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "Ahora, subimos este archivo a nuestro bucket. Creamos una carpeta llamada `raw/` para indicar que son datos \"crudos\" o sin procesar.\n",
    "\n",
    "* **Para Terminales Bash (Linux/macOS/Git Bash):**\n",
    "```bash\n",
    "# Reemplaza con el nombre de tu bucket de la celda anterior\n",
    "BUCKET_NAME=\"mlops-data-lake-tutorial-1756515865\"\n",
    "gsutil cp iris_raw.csv gs://$BUCKET_NAME/raw/iris_data/\n",
    "echo \"Archivo subido. Verificando el contenido del bucket...\"\n",
    "gsutil ls gs://$BUCKET_NAME/raw/iris_data/\n",
    "```\n",
    "\n",
    "* **Para Terminal de Windows (CMD):**\n",
    "```bat\n",
    "set BUCKET_NAME=mlops-data-lake-tutorial-tu-nombre-unico\n",
    "gsutil cp iris_raw.csv gs://%BUCKET_NAME%/raw/iris_data/\n",
    "echo Archivo subido. Verificando el contenido del bucket...\n",
    "gsutil ls gs://%BUCKET_NAME%/raw/iris_data/\n",
    "```\n",
    "**Explicación:**\n",
    "- `gsutil cp`: Copia un archivo desde tu máquina local a GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de los Datos para el Modelo\n",
    "\n",
    "En un proyecto real, se procesarían los datos para limpiarlos y prepararlos para el modelo. Aquí, haremos un pequeño cambio a las columnas y guardaremos el resultado en una nueva carpeta, `curated/`, que contiene datos 'curados' o listos para usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# 1. Define tu bucket y las rutas de los archivos\n",
    "BUCKET_NAME = \"mlops-data-lake-tutorial-1756515865\"  # Reemplaza con el nombre de tu bucket\n",
    "RAW_FILE_PATH = \"raw/iris_data/iris_raw.csv\"\n",
    "CURATED_FILE_PATH = \"curated/iris_data/iris_processed.csv\"\n",
    "\n",
    "# 2. Descargar el archivo crudo desde GCS\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(RAW_FILE_PATH)\n",
    "blob.download_to_filename(\"iris_raw.csv\")\n",
    "\n",
    "# 3. Leer y procesar los datos con Pandas\n",
    "df_raw = pd.read_csv(\"iris_raw.csv\")\n",
    "df_processed = df_raw.rename(columns={\n",
    "    'sepal length (cm)': 'sepal_length',\n",
    "    'sepal width (cm)': 'sepal_width',\n",
    "    'petal length (cm)': 'petal_length',\n",
    "    'petal width (cm)': 'petal_width'\n",
    "})\n",
    "\n",
    "# 4. Guardar los datos procesados localmente\n",
    "df_processed.to_csv(\"iris_processed.csv\", index=False)\n",
    "\n",
    "# 5. Subir el archivo procesado a la carpeta 'curated' en GCS\n",
    "blob = bucket.blob(CURATED_FILE_PATH)\n",
    "blob.upload_from_filename(\"iris_processed.csv\")\n",
    "\n",
    "print(\"Datos procesados y subidos a la capa 'curated' del Data Lake.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento y Versionado del Modelo\n",
    "\n",
    "En MLOps, es crucial guardar los modelos de manera organizada y versionada. Así podemos saber qué modelo se usó para una predicción específica. Entrenaremos un modelo `RandomForestClassifier` y lo guardaremos con una etiqueta de versión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define tu bucket y el archivo de datos procesados\n",
    "BUCKET_NAME = \"mlops-data-lake-tutorial-1756515865\" # Reemplaza con el nombre de tu bucket\n",
    "CURATED_FILE_PATH = \"curated/iris_data/iris_processed.csv\"\n",
    "\n",
    "# 2. Descargar los datos procesados para el entrenamiento\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(CURATED_FILE_PATH)\n",
    "blob.download_to_filename(\"iris_processed.csv\")\n",
    "\n",
    "# 3. Preparar los datos para el modelo\n",
    "df_curated = pd.read_csv(\"iris_processed.csv\")\n",
    "X = df_curated.drop(columns=['target'])\n",
    "y = df_curated['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Entrenar el modelo de Machine Learning\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Guardar el modelo entrenado localmente\n",
    "model_filename = 'iris_model.joblib'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "print(\"Modelo entrenado y guardado localmente.\")\n",
    "\n",
    "# 6. Subir el modelo a una carpeta versionada en GCS\n",
    "VERSION = 'v1.0' # Esta es la versión de nuestro modelo\n",
    "MODEL_GCS_PATH = f\"models/{VERSION}/{model_filename}\"\n",
    "blob = bucket.blob(MODEL_GCS_PATH)\n",
    "blob.upload_from_filename(model_filename)\n",
    "\n",
    "print(f\"Modelo versionado subido a: gs://{BUCKET_NAME}/{MODEL_GCS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Despliegue del Modelo en Cloud Run\n",
    "\n",
    "Ahora que tenemos nuestro modelo guardado, lo haremos accesible como un servicio web. Usaremos **Docker** para empaquetar nuestra aplicación y **Cloud Run** para ejecutarla de manera escalable y sin servidor. \n",
    "\n",
    "### 4.1. Crear los Archivos de la Aplicación\n",
    "\n",
    "Necesitamos tres archivos en una misma carpeta en tu máquina local. Estos archivos le dirán a Docker cómo crear una imagen de nuestra aplicación y qué debe hacer la aplicación.\n",
    "\n",
    "**Archivo 1: `main.py`** (la lógica de la API)\n",
    "Copia este código y guárdalo como `main.py`.\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Descargar el modelo desde GCS cuando se inicie la aplicación\n",
    "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH')\n",
    "LOCAL_MODEL_PATH = '/tmp/iris_model.joblib'\n",
    "\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "    blob.download_to_filename(LOCAL_MODEL_PATH)\n",
    "\n",
    "model = joblib.load(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Define la estructura de datos que esperamos para la predicción\n",
    "class Item(BaseModel):\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "\n",
    "# Un endpoint para verificar que el servicio está funcionando\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "# El endpoint principal para realizar predicciones\n",
    "@app.post(\"/predict\")\n",
    "def predict(item: Item):\n",
    "    data = [[item.sepal_length, item.sepal_width, item.petal_length, item.petal_width]]\n",
    "    prediction = model.predict(data)\n",
    "    return {\"prediction\": int(prediction[0])}\n",
    "```\n",
    "\n",
    "**Archivo 2: `requirements.txt`** (las dependencias de Python)\n",
    "Copia este código y guárdalo como `requirements.txt`.\n",
    "\n",
    "```text\n",
    "fastapi\n",
    "uvicorn\n",
    "scikit-learn\n",
    "joblib\n",
    "pandas\n",
    "google-cloud-storage\n",
    "pydantic\n",
    "gunicorn\n",
    "```\n",
    "\n",
    "**Archivo 3: `Dockerfile`** (las instrucciones para Docker)\n",
    "Copia este código y guárdalo como `Dockerfile` (sin extensión).\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt ./requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY main.py .\n",
    "CMD exec gunicorn --bind :$PORT --workers 1 --worker-class uvicorn.workers.UvicornWorker main:app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Construir la Imagen de Docker y Desplegarla\n",
    "\n",
    "Ahora, desde tu terminal, navega a la carpeta donde guardaste los archivos anteriores y ejecuta los siguientes comandos. No olvides reemplazar las variables con tus valores.\n",
    "\n",
    "* **Para Terminales Bash (Linux/macOS/Git Bash):**\n",
    "```bash\n",
    "# 1. Define tus variables (reemplaza con tu información)\n",
    "PROJECT_ID=\"el-id-de-tu-proyecto\"\n",
    "BUCKET_NAME=\"mlops-data-lake-tutorial-1756515865\"\n",
    "MODEL_PATH=\"models/v1.0/iris_model.joblib\"\n",
    "SERVICE_NAME=\"iris-predictor\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "# 2. Construir la imagen de Docker y subirla a Google Container Registry\n",
    "gcloud builds submit . --tag gcr.io/$PROJECT_ID/$SERVICE_NAME\n",
    "\n",
    "# 3. Desplegar el servicio en Cloud Run\n",
    "gcloud run deploy $SERVICE_NAME \\\n",
    "    --image gcr.io/$PROJECT_ID/$SERVICE_NAME \\\n",
    "    --platform managed \\\n",
    "    --region $REGION \\\n",
    "    --no-allow-unauthenticated \\\n",
    "    --set-env-vars BUCKET_NAME=$BUCKET_NAME,MODEL_PATH=$MODEL_PATH\n",
    "```\n",
    "\n",
    "* **Para Terminal de Windows (CMD):**\n",
    "```bat\n",
    "rem 1. Define tus variables (reemplaza con tu información)\n",
    "set PROJECT_ID=el-id-de-tu-proyecto\n",
    "set BUCKET_NAME=mlops-data-lake-tutorial-tu-nombre-unico\n",
    "set MODEL_PATH=models/v1.0/iris_model.joblib\n",
    "set SERVICE_NAME=iris-predictor\n",
    "set REGION=us-central1\n",
    "\n",
    "rem 2. Construir la imagen de Docker y subirla a Google Container Registry\n",
    "gcloud builds submit . --tag gcr.io/%PROJECT_ID%/%SERVICE_NAME%\n",
    "\n",
    "rem 3. Desplegar el servicio en Cloud Run (el comando es el mismo)\n",
    "gcloud run deploy %SERVICE_NAME% ^\n",
    "    --image gcr.io/%PROJECT_ID%/%SERVICE_NAME% ^\n",
    "    --platform managed ^\n",
    "    --region %REGION% ^\n",
    "    --no-allow-unauthenticated ^\n",
    "    --set-env-vars BUCKET_NAME=%BUCKET_NAME%,MODEL_PATH=%MODEL_PATH%\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "- `gcloud builds submit`: Toma tu `Dockerfile` y tu código, construye la imagen del contenedor y la sube al registro de imágenes de Google.\n",
    "- **(Corrección):** El punto (`.`) indica que el código fuente para la construcción está en el directorio actual. Esto corrige el error de \"Dockerfile required\".\n",
    "- `gcloud run deploy`: Toma la imagen del contenedor y la despliega en Cloud Run, creando un servicio web. Le pasamos las variables de entorno para que la aplicación sepa dónde encontrar el modelo en nuestro Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Probar el Endpoint de Predicción\n",
    "\n",
    "Una vez que el despliegue esté completo, Cloud Run te proporcionará una URL para tu servicio. Vamos a obtenerla y a hacer una prueba.\n",
    "\n",
    "* **Para Terminales Bash (Linux/macOS/Git Bash):**\n",
    "```bash\n",
    "# 1. Obtener la URL del servicio desplegado\n",
    "SERVICE_URL=$(gcloud run services describe $SERVICE_NAME --platform managed --region $REGION --format 'value(status.url)')\n",
    "echo \"URL del servicio: $SERVICE_URL\"\n",
    "\n",
    "# 2. Obtener un token de autenticación (necesario para servicios no públicos)\n",
    "TOKEN=$(gcloud auth print-identity-token)\n",
    "\n",
    "# 3. Hacer una solicitud POST para obtener una predicción\n",
    "curl -X POST \"$SERVICE_URL/predict\" \\\n",
    "    -H \"Authorization: Bearer $TOKEN\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}'\n",
    "```\n",
    "\n",
    "* **Para Terminal de Windows (CMD):**\n",
    "```bat\n",
    "rem 1. Obtener la URL del servicio desplegado\n",
    "rem Este comando de una sola línea usa 'for' para capturar la URL y la asigna a la variable SERVICE_URL\n",
    "rem Opcional gcloud run services describe %SERVICE_NAME% --platform managed --region %REGION% --format \"value(status.url)\" y crear SERVICE_URL copiando la URL\n",
    "for /f \"delims=\" %%a in ('gcloud run services describe %SERVICE_NAME% --platform managed --region %REGION% --format \"value(status.url)\"') do set \"SERVICE_URL=%%a\"\n",
    "echo URL del servicio: %SERVICE_URL%\n",
    "\n",
    "rem 2. Obtener un token de autenticación (necesario para servicios no públicos)\n",
    "rem Similar al anterior, este comando captura el token y lo asigna a la variable TOKEN\n",
    "rem Opcional gcloud auth print-identity-token y crear TOKEN copiando la Url\n",
    "\n",
    "for /f \"delims=\" %%a in ('gcloud auth print-identity-token') do set \"TOKEN=%%a\"\n",
    "\n",
    "rem 3. Hacer una solicitud POST para obtener una predicción\n",
    "curl -X POST \"%SERVICE_URL%/predict\" -H \"Authorization: Bearer %TOKEN%\" -H \"Content-Type: application/json\" -d \"{\\\"sepal_length\\\": 5.1, \\\"sepal_width\\\": 3.5, \\\"petal_length\\\": 1.4, \\\"petal_width\\\": 0.2}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "¡Felicidades! Has completado tu primer pipeline de MLOps en GCP. Desde el almacenamiento de datos en un Data Lake, pasando por el entrenamiento del modelo, hasta su despliegue como un servicio web escalable. \n",
    "\n",
    "Este flujo de trabajo demuestra los principios clave de MLOps: **reproducibilidad**, **automatización** y **escalabilidad**, que son cruciales para llevar modelos de Machine Learning a producción de manera efectiva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
